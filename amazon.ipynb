{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import face_recognition\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "# from uuid import uuid4\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import asyncio\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Connect to Qdrant instance via gRPC (port 6334)\n",
    "# client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "\n",
    "# # Define collection parameters\n",
    "# collection_name = \"embedding_collection1\"\n",
    "# vector_size = 128  # Define the size of your embedding vector\n",
    "# distance_metric = Distance.COSINE  # Choose a distance metric (COSINE, EUCLIDEAN, etc.)\n",
    "\n",
    "# # Create the collection (if it doesn't exist)\n",
    "# def create():\n",
    "#     client.recreate_collection(\n",
    "#         collection_name=collection_name,\n",
    "#         vectors_config=VectorParams(size=vector_size, distance=distance_metric)\n",
    "#     )\n",
    "\n",
    "# # Function to insert data into Qdrant\n",
    "# def insert_data(name, embedding):\n",
    "#     id = str(uuid4())\n",
    "#     points = [PointStruct(id=id, vector=embedding, payload={\"name\": name})]\n",
    "#     client.upsert(\n",
    "#         collection_name=collection_name,\n",
    "#         points=points\n",
    "#     )\n",
    "#     print(f\"Embedding data with ID {id} successfully added to Qdrant.\")\n",
    "\n",
    "# # Function to perform a similarity search\n",
    "# def compare(embedding, top_k=1):\n",
    "#     threshold = 0.93\n",
    "#     search_results = client.search(\n",
    "#         collection_name=collection_name,\n",
    "#         query_vector=embedding,\n",
    "#         limit=top_k  # Number of nearest neighbors to retrieve\n",
    "#     )\n",
    "#     if len(search_results) == 0:\n",
    "#         return \"Unknown\"\n",
    "#     result = search_results[0]\n",
    "#     if result.score >= threshold:\n",
    "#         return result.payload[\"name\"]\n",
    "#     else:\n",
    "#         return \"Unknown\"\n",
    "\n",
    "# # Function to get embedding for a given image path\n",
    "# def get_embedding(path):\n",
    "#     img = cv2.imread(path)\n",
    "#     convert_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     face_locations = face_recognition.face_locations(convert_rgb)\n",
    "#     if len(face_locations) == 0:\n",
    "#         return []\n",
    "#     face_encodings = face_recognition.face_encodings(convert_rgb, known_face_locations=face_locations)\n",
    "#     return face_encodings\n",
    "\n",
    "# # Function to train data\n",
    "# def train_data(path):\n",
    "#     person_embeddings = {}\n",
    "#     if os.path.exists(path):\n",
    "#         for root, dirs, files in tqdm(os.walk(path), desc=\"Training Data\"):\n",
    "#             folder_name = os.path.basename(root)  # Get the folder name as the person name\n",
    "#             person_embeddings[folder_name] = []  # Initialize an empty list for each person\n",
    "#             for file in tqdm(files, desc=\"Processing Files\"):\n",
    "#                 full_path = os.path.join(root, file)\n",
    "#                 embeddings = get_embedding(path=full_path)\n",
    "#                 if len(embeddings) > 0:\n",
    "#                     person_embeddings[folder_name].append(embeddings[0])\n",
    "#         for person, embeddings_list in person_embeddings.items():\n",
    "#             if embeddings_list:  # Check if there are embeddings for this person\n",
    "#                 avg_embedding = np.mean(embeddings_list, axis=0)\n",
    "#                 insert_data(person, avg_embedding)\n",
    "\n",
    "# def compare_face_encoding(face_encoding, face_location):\n",
    "#     result = compare(face_encoding)\n",
    "#     return result, face_location\n",
    "\n",
    "# async def predict(frame, distance_threshold=0.2):\n",
    "#     fra = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     face_locations = face_recognition.face_locations(fra)\n",
    "#     if len(face_locations) == 0:\n",
    "#         return []\n",
    "#     face_encodings = face_recognition.face_encodings(fra, known_face_locations=face_locations)\n",
    "#     predictions = []\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         future_to_face = {executor.submit(compare_face_encoding, face_encoding, face_location): face_location \n",
    "#                           for face_encoding, face_location in zip(face_encodings, face_locations)}\n",
    "#         for future in as_completed(future_to_face):\n",
    "#             result, location = future.result()\n",
    "#             predictions.append((result, location))\n",
    "#     return predictions\n",
    "\n",
    "# def show_predictions_on_frame(frame, predictions):\n",
    "#     for name, (top, right, bottom, left) in predictions:\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     create()\n",
    "#     train_data(path=r\"C:\\Users\\sijan\\Desktop\\jojo\\yoyo\")  # Change this to your actual directory path\n",
    "\n",
    "#     video_capture = cv2.VideoCapture(0)\n",
    "#     loop = asyncio.get_event_loop()  # Get the existing event loop\n",
    "#     while True:\n",
    "#         ret, frame = video_capture.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         predictions = loop.run_until_complete(predict(frame))  # Use run_until_complete() to run the coroutine\n",
    "#         show_predictions_on_frame(frame, predictions)\n",
    "#         cv2.imshow('Webcam', frame)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     video_capture.release()\n",
    "#     cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import face_recognition\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "# from uuid import uuid4\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from tqdm import tqdm\n",
    "# import asyncio\n",
    "# import nest_asyncio  # Fix for running asyncio in Jupyter or other environments\n",
    "\n",
    "# nest_asyncio.apply()  # Allow asyncio to run in a running event loop\n",
    "\n",
    "# # Connect to Qdrant instance via gRPC (port 6334)\n",
    "# client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "\n",
    "# # Define collection parameters\n",
    "# collection_name = \"embedding_collection1\"\n",
    "# vector_size = 128  # Define the size of your embedding vector\n",
    "# distance_metric = Distance.COSINE  # Choose a distance metric (COSINE, EUCLIDEAN, etc.)\n",
    "\n",
    "# # Create the collection (if it doesn't exist)\n",
    "# def create():\n",
    "#     if not client.collection_exists(collection_name=collection_name):\n",
    "#         client.create_collection(\n",
    "#             collection_name=collection_name,\n",
    "#             vectors_config=VectorParams(size=vector_size, distance=distance_metric)\n",
    "#         )\n",
    "#         print(f\"Collection '{collection_name}' created successfully.\")\n",
    "#     else:\n",
    "#         print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "# # Function to insert data into Qdrant\n",
    "# def insert_data(name, embedding):\n",
    "#     id = str(uuid4())\n",
    "#     points = [PointStruct(id=id, vector=embedding, payload={\"name\": name})]\n",
    "#     client.upsert(\n",
    "#         collection_name=collection_name,\n",
    "#         points=points\n",
    "#     )\n",
    "#     print(f\"Embedding data with ID {id} successfully added to Qdrant.\")\n",
    "\n",
    "# # Function to perform a similarity search\n",
    "# def compare(embedding, top_k=1):\n",
    "#     threshold = 0.93\n",
    "#     search_results = client.search(\n",
    "#         collection_name=collection_name,\n",
    "#         query_vector=embedding,\n",
    "#         limit=top_k\n",
    "#     )\n",
    "#     if len(search_results) == 0:\n",
    "#         return \"Unknown\"\n",
    "#     result = search_results[0]\n",
    "#     if result.score >= threshold:\n",
    "#         return result.payload[\"name\"]\n",
    "#     else:\n",
    "#         return \"Unknown\"\n",
    "\n",
    "# # Function to get embedding for a given image path\n",
    "# def get_embedding(path):\n",
    "#     img = cv2.imread(path)\n",
    "#     convert_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     face_locations = face_recognition.face_locations(convert_rgb)\n",
    "#     if len(face_locations) == 0:\n",
    "#         return []\n",
    "#     face_encodings = face_recognition.face_encodings(convert_rgb, known_face_locations=face_locations)\n",
    "#     return face_encodings\n",
    "\n",
    "# # Function to train data\n",
    "# def train_data(path):\n",
    "#     person_embeddings = {}\n",
    "#     if os.path.exists(path):\n",
    "#         for root, dirs, files in tqdm(os.walk(path), desc=\"Training Data\"):\n",
    "#             folder_name = os.path.basename(root)  # Get the folder name as the person name\n",
    "#             embeddings_list = []  # Collect embeddings for averaging\n",
    "#             for file in tqdm(files, desc=f\"Processing Files in {folder_name}\"):\n",
    "#                 full_path = os.path.join(root, file)\n",
    "#                 embeddings = get_embedding(path=full_path)\n",
    "#                 if len(embeddings) > 0:\n",
    "#                     embeddings_list.append(embeddings[0])\n",
    "#             if embeddings_list:  # Calculate average embedding if there are valid embeddings\n",
    "#                 avg_embedding = np.mean(embeddings_list, axis=0)\n",
    "#                 insert_data(folder_name, avg_embedding)\n",
    "\n",
    "# def compare_face_encoding(face_encoding, face_location):\n",
    "#     result = compare(face_encoding)\n",
    "#     return result, face_location\n",
    "\n",
    "# async def predict(frame, distance_threshold=0.2):\n",
    "#     fra = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     face_locations = face_recognition.face_locations(fra)\n",
    "#     if len(face_locations) == 0:\n",
    "#         return []\n",
    "#     face_encodings = face_recognition.face_encodings(fra, known_face_locations=face_locations)\n",
    "#     predictions = []\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         future_to_face = {executor.submit(compare_face_encoding, face_encoding, face_location): face_location \n",
    "#                           for face_encoding, face_location in zip(face_encodings, face_locations)}\n",
    "#         for future in as_completed(future_to_face):\n",
    "#             result, location = future.result()\n",
    "#             predictions.append((result, location))\n",
    "#     return predictions\n",
    "\n",
    "# def show_predictions_on_frame(frame, predictions):\n",
    "#     for name, (top, right, bottom, left) in predictions:\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "# async def process_video():\n",
    "#     video_capture = cv2.VideoCapture(0)\n",
    "#     while True:\n",
    "#         ret, frame = video_capture.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         predictions = await predict(frame)\n",
    "#         show_predictions_on_frame(frame, predictions)\n",
    "#         cv2.imshow('Webcam', frame)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     video_capture.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     create()\n",
    "#     train_data(path=r\"C:\\Users\\sijan\\Desktop\\jojo\\yoyo\")  # Change this to your actual directory path\n",
    "#     asyncio.run(process_video())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import face_recognition\n",
    "\n",
    "# def process_images(input_dir, output_dir):\n",
    "#     # Create the output directory if it doesn't exist\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     for root, dirs, files in os.walk(input_dir):\n",
    "#         for file in files:\n",
    "#             if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "#                 input_path = os.path.join(root, file)\n",
    "\n",
    "#                 # Create corresponding output folder structure\n",
    "#                 relative_path = os.path.relpath(root, input_dir)\n",
    "#                 output_subdir = os.path.join(output_dir, relative_path)\n",
    "#                 if not os.path.exists(output_subdir):\n",
    "#                     os.makedirs(output_subdir)\n",
    "\n",
    "#                 output_path = os.path.join(output_subdir, file)\n",
    "\n",
    "#                 # Process the image\n",
    "#                 image = cv2.imread(input_path)\n",
    "#                 rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#                 face_locations = face_recognition.face_locations(rgb_image, model='hog')\n",
    "\n",
    "#                 for top, right, bottom, left in face_locations:\n",
    "#                     # Expand the face box to include head to chin with ears\n",
    "#                     height = bottom - top\n",
    "#                     width = right - left\n",
    "#                     margin = int(0.2 * height)\n",
    "\n",
    "#                     extended_top = max(0, top - margin)\n",
    "#                     extended_bottom = min(image.shape[0], bottom + margin)\n",
    "#                     extended_left = max(0, left - margin)\n",
    "#                     extended_right = min(image.shape[1], right + margin)\n",
    "\n",
    "#                     face_crop = image[extended_top:extended_bottom, extended_left:extended_right]\n",
    "\n",
    "#                     # Save the cropped face\n",
    "#                     cv2.imwrite(output_path, face_crop)\n",
    "#                 else:\n",
    "#                     print(f\"No face found in {input_path}, skipping.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_folder = r\"C:\\Users\\sijan\\Desktop\\jojo\"  # Replace with the input folder path\n",
    "#     output_folder = r\"C:\\Users\\sijan\\Desktop\\fake_jojo\"  # Replace with the output folder path\n",
    "#     process_images(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "# from uuid import uuid4\n",
    "# from tqdm import tqdm\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# import torch\n",
    "\n",
    "# # Connect to Qdrant instance\n",
    "# client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "\n",
    "# # Collection parameters\n",
    "# collection_name = \"embedding_collection1\"\n",
    "# vector_size = 512  # CLIP model outputs embeddings of size 512\n",
    "# distance_metric = Distance.COSINE\n",
    "\n",
    "# # Initialize CLIP model and processor\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# # Create the collection\n",
    "# def create_collection():\n",
    "#     if not client.collection_exists(collection_name=collection_name):\n",
    "#         client.create_collection(\n",
    "#             collection_name=collection_name,\n",
    "#             vectors_config=VectorParams(size=vector_size, distance=distance_metric),\n",
    "#         )\n",
    "#         print(f\"Collection '{collection_name}' created successfully.\")\n",
    "#     else:\n",
    "#         print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "# # Insert data into Qdrant\n",
    "# def insert_data(name, embedding):\n",
    "#     id = str(uuid4())\n",
    "#     points = [PointStruct(id=id, vector=embedding.tolist(), payload={\"name\": name})]\n",
    "#     client.upsert(collection_name=collection_name, points=points)\n",
    "#     print(f\"Added embedding for {name} with ID {id}.\")\n",
    "\n",
    "# # Extract embeddings for an image using CLIP\n",
    "# def get_embedding(image_path):\n",
    "#     image = cv2.imread(image_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "#     with torch.no_grad():\n",
    "#         embedding = model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "#     return embedding\n",
    "\n",
    "# # Train embeddings for the dataset\n",
    "# def train_data(dataset_path):\n",
    "#     if not os.path.exists(dataset_path):\n",
    "#         print(\"Dataset path does not exist.\")\n",
    "#         return\n",
    "    \n",
    "#     for root, dirs, files in tqdm(os.walk(dataset_path), desc=\"Processing Folders\"):\n",
    "#         folder_name = os.path.basename(root)\n",
    "#         embeddings_list = []\n",
    "        \n",
    "#         for file in tqdm(files, desc=f\"Processing Files in {folder_name}\"):\n",
    "#             full_path = os.path.join(root, file)\n",
    "#             embedding = get_embedding(full_path)\n",
    "#             embeddings_list.append(embedding)\n",
    "        \n",
    "#         if embeddings_list:\n",
    "#             avg_embedding = np.mean(embeddings_list, axis=0)\n",
    "#             insert_data(folder_name, avg_embedding)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     create_collection()\n",
    "#     train_data(dataset_path=r\"C:\\Users\\sijan\\Desktop\\si\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "# from qdrant_client import QdrantClient\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# import torch\n",
    "\n",
    "# nest_asyncio.apply()  # Allow asyncio in environments with a running event loop\n",
    "\n",
    "# # Connect to Qdrant instance\n",
    "# client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "# collection_name = \"embedding_collection1\"\n",
    "\n",
    "# # Initialize CLIP model and processor\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# # Compare embeddings against the stored ones in Qdrant\n",
    "# def compare(embedding, top_k=1):\n",
    "#     threshold = 0.93\n",
    "#     search_results = client.search(\n",
    "#         collection_name=collection_name,\n",
    "#         query_vector=embedding.tolist(),\n",
    "#         limit=top_k\n",
    "#     )\n",
    "#     if not search_results:\n",
    "#         return \"Unknown\"\n",
    "#     result = search_results[0]\n",
    "#     if result.score >= threshold:\n",
    "#         return result.payload[\"name\"]\n",
    "#     else:\n",
    "#         return \"Unknown\"\n",
    "\n",
    "# # Extract embeddings for a frame using CLIP\n",
    "# def get_embedding(frame):\n",
    "#     rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     inputs = processor(images=rgb_frame, return_tensors=\"pt\", padding=True)\n",
    "#     with torch.no_grad():\n",
    "#         embedding = model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "#     return embedding\n",
    "\n",
    "# # Predict face recognition for the current frame\n",
    "# async def predict(frame, executor):\n",
    "#     face_locations = face_recognition.face_locations(frame)\n",
    "#     if not face_locations:\n",
    "#         return []\n",
    "\n",
    "#     embeddings = [get_embedding(frame) for _ in face_locations]  # Assuming the whole frame for each detected face\n",
    "#     predictions = []\n",
    "\n",
    "#     future_to_face = {\n",
    "#         executor.submit(compare, embedding): face_location\n",
    "#         for embedding, face_location in zip(embeddings, face_locations)\n",
    "#     }\n",
    "#     for future in as_completed(future_to_face):\n",
    "#         result = future.result()\n",
    "#         location = future_to_face[future]\n",
    "#         predictions.append((result, location))\n",
    "\n",
    "#     return predictions\n",
    "\n",
    "# # Display predictions on the webcam feed\n",
    "# def show_predictions_on_frame(frame, predictions):\n",
    "#     for name, (top, right, bottom, left) in predictions:\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "# # Process the webcam feed\n",
    "# async def process_video():\n",
    "#     video_capture = cv2.VideoCapture(0)\n",
    "#     frame_skip = 5  # Process every 5th frame\n",
    "#     frame_count = 0\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "#         while True:\n",
    "#             ret, frame = video_capture.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#             frame_count += 1\n",
    "#             if frame_count % frame_skip == 0:\n",
    "#                 predictions = await predict(frame, executor)\n",
    "#                 show_predictions_on_frame(frame, predictions)\n",
    "\n",
    "#             # Display the frame\n",
    "#             cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "#             # Exit on 'q' key press\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "\n",
    "#     video_capture.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(process_video())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Connect to Qdrant instance\n",
    "client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "\n",
    "# Collection parameters\n",
    "collection_name = \"embedding_collection1\"\n",
    "vector_size = 128  # Vector size is model-dependent; adjust as needed\n",
    "distance_metric = Distance.COSINE\n",
    "\n",
    "# Load Dlib models\n",
    "face_detector = dlib.get_frontal_face_detector()  # Face detector\n",
    "shape_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Pre-trained landmarks model\n",
    "face_rec_model = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1.dat\")  # Pre-trained face embeddings model\n",
    "\n",
    "# Create the collection\n",
    "def create_collection():\n",
    "    if not client.collection_exists(collection_name=collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=distance_metric),\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "# Insert data into Qdrant\n",
    "def insert_data(name, embedding):\n",
    "    id = str(uuid4())\n",
    "    points = [PointStruct(id=id, vector=embedding.tolist(), payload={\"name\": name})]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "    print(f\"Added embedding for {name} with ID {id}.\")\n",
    "\n",
    "# Preprocessing: Align and resize face\n",
    "def preprocess_face(image, landmarks):\n",
    "    # Align the face based on eye landmarks\n",
    "    left_eye = np.mean(landmarks[36:42], axis=0)  # Indices for left eye in Dlib's 68-point model\n",
    "    right_eye = np.mean(landmarks[42:48], axis=0)  # Indices for right eye\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    angle = np.degrees(np.arctan2(dy, dx))\n",
    "    center = tuple(map(int, np.mean([left_eye, right_eye], axis=0)))  # Convert to tuple of integers\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    aligned_face = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "    return aligned_face\n",
    "\n",
    "\n",
    "# Extract embeddings for an image\n",
    "def get_embedding(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    detections = face_detector(rgb_image, 1)  # Detect faces with Dlib\n",
    "\n",
    "    embeddings = []\n",
    "    for detection in detections:\n",
    "        # Get landmarks for face\n",
    "        shape = shape_predictor(rgb_image, detection)\n",
    "        landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "        # Preprocess and align the face\n",
    "        aligned_face = preprocess_face(rgb_image, landmarks)\n",
    "\n",
    "        # Get face embedding\n",
    "        embedding = face_rec_model.compute_face_descriptor(aligned_face, shape)\n",
    "        embeddings.append(np.array(embedding))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Train embeddings for the dataset\n",
    "def train_data(dataset_path):\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Dataset path does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for root, dirs, files in tqdm(os.walk(dataset_path), desc=\"Processing Folders\"):\n",
    "        folder_name = os.path.basename(root)\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing Files in {folder_name}\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            embeddings = get_embedding(full_path)\n",
    "            if embeddings:\n",
    "                embeddings_list.extend(embeddings)  # Collect all embeddings for the folder\n",
    "        \n",
    "        if embeddings_list:\n",
    "            avg_embedding = np.mean(embeddings_list, axis=0)  # Average all embeddings for the folder\n",
    "            insert_data(folder_name, avg_embedding)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_collection()\n",
    "    train_data(dataset_path=r\"C:\\Users\\sijan\\Desktop\\jojo\\yoyo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from qdrant_client import QdrantClient\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "nest_asyncio.apply()  # Allow asyncio in environments with a running event loop\n",
    "\n",
    "# Connect to Qdrant instance\n",
    "client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "collection_name = \"embedding_collection1\"\n",
    "\n",
    "# Compare embeddings against the stored ones in Qdrant\n",
    "def compare(embedding, top_k=1):\n",
    "    threshold = 0.93\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    if not search_results:\n",
    "        return \"Unknown\"\n",
    "    result = search_results[0]\n",
    "    if result.score >= threshold:\n",
    "        return result.payload[\"name\"]\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Process face detection and recognition\n",
    "def detect_and_recognize_faces(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    if not face_locations:\n",
    "        return []\n",
    "\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, known_face_locations=face_locations)\n",
    "    results = [(compare(encoding), location) for encoding, location in zip(face_encodings, face_locations)]\n",
    "    return results\n",
    "\n",
    "# Display predictions on the frame\n",
    "def show_predictions_on_frame(frame, predictions):\n",
    "    for name, (top, right, bottom, left) in predictions:\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "# Asynchronous video processing\n",
    "async def process_video():\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    frame_skip = 1  # Process every nth frame\n",
    "    frame_count = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust max_workers based on system capability\n",
    "        loop = asyncio.get_event_loop()\n",
    "\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            if frame_count % frame_skip == 0:\n",
    "                # Run face detection and recognition in a thread\n",
    "                predictions = await loop.run_in_executor(executor, detect_and_recognize_faces, frame)\n",
    "                show_predictions_on_frame(frame, predictions)\n",
    "\n",
    "            # Display the frame\n",
    "            cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "            # Exit on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main entry point\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(process_video())\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from deepface import DeepFace\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "# from uuid import uuid4\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Connect to Qdrant instance\n",
    "# client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "\n",
    "# # Collection parameters\n",
    "# collection_name = \"embedding_collection1\"\n",
    "# vector_size = 4096  # Reduced to 128-dimensional embedding\n",
    "# distance_metric = Distance.COSINE\n",
    "\n",
    "# # Create the collection\n",
    "# def create_collection():\n",
    "#     if not client.collection_exists(collection_name=collection_name):\n",
    "#         client.create_collection(\n",
    "#             collection_name=collection_name,\n",
    "#             vectors_config=VectorParams(size=vector_size, distance=distance_metric),\n",
    "#         )\n",
    "#         print(f\"Collection '{collection_name}' created successfully.\")\n",
    "#     else:\n",
    "#         print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "# # Insert data into Qdrant\n",
    "# def insert_data(name, embedding):\n",
    "#     id = str(uuid4())\n",
    "#     # Ensure embedding is exactly 128 dimensions\n",
    "#     # embedding = embedding[:128]\n",
    "#     points = [PointStruct(id=id, vector=embedding, payload={\"name\": name})]\n",
    "#     client.upsert(collection_name=collection_name, points=points)\n",
    "#     print(f\"Added embedding for {name} with ID {id}.\")\n",
    "\n",
    "# # Extract embeddings for an image using DeepFace\n",
    "# def get_embedding(image_path):\n",
    "#     try:\n",
    "#         # Use DeepFace to extract face embedding\n",
    "#         # Reduce embedding to 128 dimensions\n",
    "#         embedding_objs = DeepFace.represent(\n",
    "#             img_path=image_path, \n",
    "#             model_name='VGG-Face',\n",
    "#             enforce_detection=False,\n",
    "#             detector_backend='opencv',  # More robust face detection\n",
    "#             align=True,  # Ensure face alignment\n",
    "#             normalization='base'  # Normalize the embedding\n",
    "#         )\n",
    "#         print(len(embedding_objs[0]['embedding']))\n",
    "#         # If multiple faces are detected, take the first embedding\n",
    "#         if embedding_objs:\n",
    "#             # Truncate or pad the embedding to exactly 128 dimensions\n",
    "#             full_embedding = embedding_objs[0]['embedding']\n",
    "#             return full_embedding\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {image_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Train embeddings for the dataset\n",
    "# def train_data(dataset_path):\n",
    "#     if not os.path.exists(dataset_path):\n",
    "#         print(\"Dataset path does not exist.\")\n",
    "#         return\n",
    "    \n",
    "#     for root, dirs, files in tqdm(os.walk(dataset_path), desc=\"Processing Folders\"):\n",
    "#         folder_name = os.path.basename(root)\n",
    "#         embeddings_list = []\n",
    "        \n",
    "#         for file in tqdm(files, desc=f\"Processing Files in {folder_name}\"):\n",
    "#             # Check for image files\n",
    "#             if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "#                 full_path = os.path.join(root, file)\n",
    "#                 embedding = get_embedding(full_path)\n",
    "#                 if embedding is not None:\n",
    "#                     embeddings_list.append(embedding)\n",
    "        \n",
    "#         # Compute average embedding if multiple faces found\n",
    "#         if embeddings_list:\n",
    "#             # avg_embedding = np.mean(embeddings_list, axis=0).tolist()\n",
    "#             # Ensure exactly 128 dimensions\n",
    "#             # avg_embedding = avg_embedding[:128] + [0] * max(0, 128 - len(avg_embedding))\n",
    "#             for embedding in embeddings_list:\n",
    "#                 insert_data(folder_name, embedding)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Ensure you have the necessary DeepFace models downloaded\n",
    "#     # You might need to run: \n",
    "#     # DeepFace.download_model('VGG-Face')\n",
    "    \n",
    "#     create_collection()\n",
    "#     train_data(dataset_path=r\"C:\\Users\\sijan\\Desktop\\jojo\\yoyo\")\n",
    "\n",
    "# # Installation notes:\n",
    "# # pip install deepface qdrant-client opencv-python tqdm numpy\n",
    "# # \n",
    "# # Additional configuration:\n",
    "# # - Uses VGG-Face model\n",
    "# # - Reduces embedding to 128 dimensions\n",
    "# # - Handles variable-length embeddings\n",
    "# # - Provides robust face detection and alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from deepface import DeepFace\n",
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "# # Connect to Qdrant instance\n",
    "# client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "# collection_name = \"embedding_collection1\"\n",
    "\n",
    "# # Compare embeddings against the stored ones in Qdrant\n",
    "# def compare_embedding(embedding, threshold=0.93):\n",
    "#     try:\n",
    "#         # Search in Qdrant collection\n",
    "#         search_results = client.search(\n",
    "#             collection_name=collection_name,\n",
    "#             query_vector=embedding,\n",
    "#             limit=1  # Top 1 result\n",
    "#         )\n",
    "        \n",
    "#         # If no results found\n",
    "#         if not search_results:\n",
    "#             return \"Unknown\"\n",
    "        \n",
    "#         # Check if similarity meets threshold\n",
    "#         result = search_results[0]\n",
    "#         print(result.payload[\"name\"])\n",
    "#         print(result.score)\n",
    "#         if result.score >= threshold:\n",
    "#             return result.payload[\"name\"]\n",
    "#         else:\n",
    "#             return \"Unknown\"\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Comparison error: {e}\")\n",
    "#         return \"Unknown\"\n",
    "\n",
    "# # Predict faces in a frame\n",
    "# def predict_faces(frame):\n",
    "#     predictions = []\n",
    "    \n",
    "#     try:\n",
    "#         # Detect faces\n",
    "#         faces = DeepFace.extract_faces(\n",
    "#             img_path=frame, \n",
    "#             enforce_detection=False,\n",
    "#             detector_backend='opencv'\n",
    "#         )\n",
    "        \n",
    "#         # Process each detected face\n",
    "#         for face in faces:\n",
    "#             # Get face region\n",
    "#             region = face['facial_area']\n",
    "#             left, top = region['x'], region['y']\n",
    "#             right, bottom = left + region['w'], top + region['h']\n",
    "            \n",
    "#             # Extract face from original frame\n",
    "#             face_img = frame[top:bottom, left:right]\n",
    "            \n",
    "#             try:\n",
    "#                 # Get face embedding\n",
    "#                 embedding_obj = DeepFace.represent(\n",
    "#                     img_path=face_img, \n",
    "#                     model_name='VGG-Face',\n",
    "#                     enforce_detection=False\n",
    "#                 )\n",
    "                \n",
    "#                 # Truncate to 128 dimensions if needed\n",
    "#                 if embedding_obj:\n",
    "#                     embedding = embedding_obj[0]['embedding']\n",
    "                    \n",
    "#                     # Compare embedding\n",
    "#                     name = compare_embedding(embedding)\n",
    "                    \n",
    "#                     # Add prediction\n",
    "#                     predictions.append({\n",
    "#                         'name': name,\n",
    "#                         'location': (top, right, bottom, left)\n",
    "#                     })\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Embedding extraction error: {e}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Face detection error: {e}\")\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "# # Visualize predictions on frame\n",
    "# def draw_predictions(frame, predictions):\n",
    "#     for pred in predictions:\n",
    "#         name = pred['name']\n",
    "#         (top, right, bottom, left) = pred['location']\n",
    "        \n",
    "#         # Draw rectangle around the face\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        \n",
    "#         # Put name label\n",
    "#         cv2.rectangle(frame, (left, bottom), (right, bottom + 35), (0, 255, 0), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom + 30), \n",
    "#                     font, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "#     return frame\n",
    "\n",
    "# # Real-time face recognition\n",
    "# def real_time_recognition():\n",
    "#     # Open webcam\n",
    "#     video_capture = cv2.VideoCapture(0)\n",
    "    \n",
    "#     while True:\n",
    "#         # Capture frame-by-frame\n",
    "#         ret, frame = video_capture.read()\n",
    "        \n",
    "#         if not ret:\n",
    "#             break\n",
    "        \n",
    "#         # Predict faces\n",
    "#         predictions = predict_faces(frame)\n",
    "        \n",
    "#         # Draw predictions\n",
    "#         annotated_frame = draw_predictions(frame, predictions)\n",
    "        \n",
    "#         # Display the resulting frame\n",
    "#         cv2.imshow('Face Recognition', annotated_frame)\n",
    "        \n",
    "#         # Exit on 'q' press\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "    \n",
    "#     # Release resources\n",
    "#     video_capture.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     real_time_recognition()\n",
    "\n",
    "# # Required dependencies:\n",
    "# # pip install deepface qdrant-client opencv-python numpy\n",
    "# #\n",
    "# # Notes:\n",
    "# # - Connects to previously created Qdrant collection\n",
    "# # - Uses DeepFace for face detection and embedding\n",
    "# # - Performs real-time face recognition\n",
    "# # - Handles multiple face detection in a single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'embedding_collection1' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files in yoyo: 0it [00:00, ?it/s]\n",
      "Processing Files in safal: 100%|██████████| 11/11 [01:43<00:00,  9.42s/it]\n",
      "Processing Folders: 2it [01:43, 51.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added embedding for safal with ID 4d968a03-40e7-4222-b0aa-fa24b78731c1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files in sijan: 100%|██████████| 11/11 [01:29<00:00,  8.13s/it]\n",
      "Processing Folders: 3it [03:13, 64.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added embedding for sijan with ID cdbbe339-bcd4-4e2c-a3d2-ac8cca68f238.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Connect to Qdrant instance\n",
    "client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "\n",
    "# Collection parameters\n",
    "collection_name = \"embedding_collection1\"\n",
    "vector_size = 128\n",
    "distance_metric = Distance.COSINE\n",
    "\n",
    "# Create the collection\n",
    "def create_collection():\n",
    "    if not client.collection_exists(collection_name=collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=distance_metric),\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "# Insert data into Qdrant\n",
    "def insert_data(name, embedding):\n",
    "    id = str(uuid4())\n",
    "    points = [PointStruct(id=id, vector=embedding, payload={\"name\": name})]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "    print(f\"Added embedding for {name} with ID {id}.\")\n",
    "\n",
    "# Extract embeddings for an image\n",
    "def get_embedding(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_image)\n",
    "    if not face_locations:\n",
    "        return []\n",
    "    face_encodings = face_recognition.face_encodings(rgb_image, known_face_locations=face_locations)\n",
    "    return face_encodings\n",
    "\n",
    "# Train embeddings for the dataset\n",
    "def train_data(dataset_path):\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Dataset path does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for root, dirs, files in tqdm(os.walk(dataset_path), desc=\"Processing Folders\"):\n",
    "        folder_name = os.path.basename(root)\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing Files in {folder_name}\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            embeddings = get_embedding(full_path)\n",
    "            if embeddings:\n",
    "                embeddings_list.append(embeddings[0])\n",
    "        \n",
    "        if embeddings_list:\n",
    "            avg_embedding = np.mean(embeddings_list, axis=0)\n",
    "            insert_data(folder_name, avg_embedding)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_collection()\n",
    "    train_data(dataset_path=r\"C:\\Users\\sijan\\Desktop\\jojo\\yoyo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from qdrant_client import QdrantClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "nest_asyncio.apply()  # Allow asyncio in environments with a running event loop\n",
    "\n",
    "# Connect to Qdrant instance\n",
    "client = QdrantClient(host=\"localhost\", port=6334, prefer_grpc=True)\n",
    "collection_name = \"embedding_collection1\"\n",
    "\n",
    "# Compare embeddings against the stored ones in Qdrant\n",
    "def compare(embedding, top_k=1):\n",
    "    threshold = 0.93\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    if not search_results:\n",
    "        return \"Unknown\"\n",
    "    result = search_results[0]\n",
    "    if result.score >= threshold:\n",
    "        return result.payload[\"name\"]\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Predict face recognition for the current frame\n",
    "async def predict(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    if not face_locations:\n",
    "        return []\n",
    "    \n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, known_face_locations=face_locations)\n",
    "    predictions = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        future_to_face = {\n",
    "            executor.submit(compare, face_encoding): face_location\n",
    "            for face_encoding, face_location in zip(face_encodings, face_locations)\n",
    "        }\n",
    "        for future in as_completed(future_to_face):\n",
    "            result = future.result()\n",
    "            location = future_to_face[future]\n",
    "            predictions.append((result, location))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Display predictions on the webcam feed\n",
    "def show_predictions_on_frame(frame, predictions):\n",
    "    for name, (top, right, bottom, left) in predictions:\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "# Process the webcam feed\n",
    "async def process_video():\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        predictions = await predict(frame)\n",
    "        show_predictions_on_frame(frame, predictions)\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(process_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luffy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
